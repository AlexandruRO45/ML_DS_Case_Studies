{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study Model selection for Clustering\n",
    " \n",
    "\n",
    "Clustering is unsupervised learning: the resulting clusters are completely derived from data distributed in given a feature set with no class available\n",
    "\n",
    "Compared to supervised learning counterparts, it is â€¦\n",
    "* hard to define model performance (cluster quality)\n",
    "* sensitive to different clustering algorithms and different feature spaces.\n",
    "\n",
    "\n",
    "\n",
    "#### Task\n",
    "Your task is to try different clustering algorithms and also a range of the potential parameter(s) which affect the number of clusters including ..\n",
    "\n",
    "* K-means\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "* Gaussian Mixture Model, \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\n",
    "* Hierarchical Clustering, \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n",
    "* Louvain Clustering, \n",
    "https://scikit-network.readthedocs.io/en/latest/reference/clustering.html#module-sknetwork.clustering\n",
    "\n",
    "on 5K colorectal patches represented by 4 different representation PathologyGAN, ResNet50, InceptionV3 and VGG16\n",
    "\n",
    "\n",
    "#### Data and its preprocessing \n",
    "5,000 non-overlapping image patches from hematoxylin & eosin (H&E) stained histological images of human colorectal cancer (CRC) and normal tissue.\n",
    "* 4 feature sets, PathologyGAN, ResNet50, InceptionV3 and VGG16, are extracted to represent those 5,000 images different dimensional feature spaces.\n",
    "* PCA and UMAP were employed to reduce each feature sapce into 100-dimensional vectors\n",
    "\n",
    "* 9 tissue types are also available which include Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM)\n",
    "\n",
    "\n",
    "#### Performance Measurement\n",
    "To assess quality of clustering solutions, several approaches are expected to be done and interpreted which include...\n",
    "* Silhouette Score for goodness of fit test\n",
    "* Vmeasure Score for homogeneity and completeness test (tissue type available as ground truth)\n",
    "* Clusters visualisations\n",
    "\n",
    "For more information, please have a check...\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n",
    "\n",
    "\n",
    "#### Report\n",
    "Report on your preprocessing pipeline, theory and intuition behinds each algorithm and representation, parameter searching and performance evaluation frameworks. If there is any addiotional process, give evidences/justifications on how it helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install h5py==2.10.0\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install sklearn\n",
    "# !pip install scikit-network\n",
    "# !pip install pickle-mixin==1.0.2\n",
    "# !pip install matplotlib\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and computation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "# Machine Learning and Data Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "\n",
    "# Clustering\n",
    "from sknetwork.clustering import Louvain\n",
    "from scipy import sparse\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Others\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pge_path = 'colon_nct_feature/pge_dim_reduced_feature.h5'\n",
    "resnet50_path = 'colon_nct_feature/resnet50_dim_reduced_feature.h5'\n",
    "inceptionv3_path = 'colon_nct_feature/inceptionv3_dim_reduced_feature.h5'\n",
    "vgg16_path = 'colon_nct_feature/vgg16_dim_reduced_feature.h5'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pge_content = h5py.File(pge_path, mode='r')\n",
    "resnet50_content = h5py.File(resnet50_path, mode='r')\n",
    "inceptionv3_content = h5py.File(inceptionv3_path, mode='r')\n",
    "vgg16_content = h5py.File(vgg16_path, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "pge_normalized = scaler.fit_transform(pge_content)\n",
    "resnet50_normalized = scaler.fit_transform(resnet50_content)\n",
    "inceptionv3_normalized = scaler.fit_transform(inceptionv3_content)\n",
    "vgg16_normalized = scaler.fit_transform(vgg16_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'pge': pge_normalized,\n",
    "    'resnet50': resnet50_normalized,\n",
    "    'inceptionv3': inceptionv3_normalized,\n",
    "    'vgg16': vgg16_normalized\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numerical data (e.g., 'pca_feature') from the h5 files\n",
    "def load_data(path, key):\n",
    "    with h5py.File(path, 'r') as h5_file:\n",
    "        data = h5_file[key][...]\n",
    "        return data\n",
    "\n",
    "# Use the 'pca_feature' or 'umap_feature' key as needed\n",
    "pge_data = load_data(pge_path, 'pca_feature')\n",
    "resnet50_data = load_data(resnet50_path, 'pca_feature')\n",
    "inceptionv3_data = load_data(inceptionv3_path, 'pca_feature')\n",
    "vgg16_data = load_data(vgg16_path, 'pca_feature')\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "pge_normalized = scaler.fit_transform(pge_data)\n",
    "resnet50_normalized = scaler.fit_transform(resnet50_data)\n",
    "inceptionv3_normalized = scaler.fit_transform(inceptionv3_data)\n",
    "vgg16_normalized = scaler.fit_transform(vgg16_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize_data(path, key):\n",
    "    with h5py.File(path, 'r') as h5_file:\n",
    "        data = h5_file[key][...]\n",
    "        return MinMaxScaler().fit_transform(data)\n",
    "\n",
    "datasets = {\n",
    "    'pge_pca': load_and_normalize_data(pge_path, 'pca_feature'),\n",
    "    'pge_umap': load_and_normalize_data(pge_path, 'umap_feature'),\n",
    "    'vgg16_pca': load_and_normalize_data(vgg16_path, 'pca_feature'),\n",
    "    'vgg16_umap': load_and_normalize_data(vgg16_path, 'umap_feature'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(h5_content):\n",
    "    filename = np.squeeze(h5_content['file_name'])\n",
    "    filename = np.array([str(x) for x in filename])\n",
    "    labels = np.array([x.split('/')[2] for x in filename])\n",
    "    return labels\n",
    "\n",
    "# Extracting labels for each dataset\n",
    "pge_labels = extract_labels(pge_content)\n",
    "resnet50_labels = extract_labels(resnet50_content)\n",
    "inceptionv3_labels = extract_labels(inceptionv3_content)\n",
    "vgg16_labels = extract_labels(vgg16_content)\n",
    "\n",
    "# You may store these labels in a dictionary for easy access\n",
    "labels_dict = {\n",
    "    'pge': pge_labels,\n",
    "    'resnet50': resnet50_labels,\n",
    "    'inceptionv3': inceptionv3_labels,\n",
    "    'vgg16': vgg16_labels\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA feature from 4 feature sets: pge_latent, resnet50_latent, inceptionv3_latent, vgg16_latent\n",
    "pge_pca_feature  = pge_content['pca_feature'][...]\n",
    "resnet50_pca_feature  = resnet50_content['pca_feature'][...]\n",
    "inceptionv3_pca_feature = inceptionv3_content['pca_feature'][...]\n",
    "vgg16_pca_feature  = vgg16_content['pca_feature'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP feature from 4 feature sets: pge_latent, resnet50_latent, inceptionv3_latent, vgg16_latent\n",
    "pge_umap_feature  = pge_content['umap_feature'][...]\n",
    "resnet50_umap_feature = resnet50_content['umap_feature'][...]\n",
    "inceptionv3_umap_feature  = inceptionv3_content['umap_feature'][...]\n",
    "vgg16_umap_feature  = vgg16_content['umap_feature'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pge_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/code/ML_DS_Case_Studies/Model selection for clustering/Getting_Started/case_study1.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#tissue type as available ground-truth: labels\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m filename  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(pge_content[\u001b[39m'\u001b[39m\u001b[39mfile_name\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m filename \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mstr\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m filename])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m filename])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pge_content' is not defined"
     ]
    }
   ],
   "source": [
    "#tissue type as available ground-truth: labels\n",
    "filename  = np.squeeze(pge_content['file_name'])\n",
    "filename = np.array([str(x) for x in filename])\n",
    "labels = np.array([x.split('/')[2] for x in filename])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Louvain clustering\n",
    "resolutions = [0.9, 1, 0.8]\n",
    "modularity_options = ['Dugue', 'Newman', 'Potts']\n",
    "random_state = 0\n",
    "\n",
    "# Function to transform distances into similarities\n",
    "def create_similarity_matrix(data):\n",
    "    distances = pairwise_distances(data)\n",
    "    # Convert distances to similarities, for example using an exponential function\n",
    "    # You can adjust this transformation as needed\n",
    "    max_distance = np.max(distances)\n",
    "    similarities = np.exp(-(distances ** 2) / (2. * max_distance ** 2))\n",
    "    return sparse.csr_matrix(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid number of unique labels: 1 for dataset pge_pca\n",
      "Invalid number of unique labels: 1 for dataset pge_pca\n",
      "Invalid number of unique labels: 1 for dataset pge_pca\n",
      "Invalid number of unique labels: 1 for dataset pge_pca\n",
      "Invalid number of unique labels: 1 for dataset pge_pca\n",
      "Invalid number of unique labels: 1 for dataset pge_umap\n",
      "Invalid number of unique labels: 1 for dataset pge_umap\n",
      "Invalid number of unique labels: 1 for dataset pge_umap\n",
      "Invalid number of unique labels: 1 for dataset pge_umap\n",
      "Invalid number of unique labels: 1 for dataset vgg16_pca\n",
      "Invalid number of unique labels: 1 for dataset vgg16_pca\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 5000. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/code/ML_DS_Case_Studies/Model selection for clustering/Getting_Started/case_study1.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Compute silhouette score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(labels)) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     silhouette \u001b[39m=\u001b[39m silhouette_score(data, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     silhouette \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# or a placeholder value, e.g., -1\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:131\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m         X, labels \u001b[39m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:277\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    275\u001b[0m n_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(labels)\n\u001b[1;32m    276\u001b[0m label_freqs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(labels)\n\u001b[0;32m--> 277\u001b[0m check_number_of_labels(\u001b[39mlen\u001b[39m(le\u001b[39m.\u001b[39mclasses_), n_samples)\n\u001b[1;32m    279\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    280\u001b[0m reduce_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    281\u001b[0m     _silhouette_reduce, labels\u001b[39m=\u001b[39mlabels, label_freqs\u001b[39m=\u001b[39mlabel_freqs\n\u001b[1;32m    282\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:37\u001b[0m, in \u001b[0;36mcheck_number_of_labels\u001b[0;34m(n_labels, n_samples)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m    Number of samples.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m n_labels \u001b[39m<\u001b[39m n_samples:\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNumber of labels is \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. Valid values are 2 to n_samples - 1 (inclusive)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39m%\u001b[39m n_labels\n\u001b[1;32m     40\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels is 5000. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "louvain_results = []\n",
    "\n",
    "for dataset_name, data in datasets.items():\n",
    "    true_labels = labels_dict[dataset_name.split('_')[0]]  # Extracting the corresponding true labels\n",
    "    similarity_matrix = create_similarity_matrix(data)\n",
    "\n",
    "    for resolution in resolutions:\n",
    "        for modularity in modularity_options:\n",
    "            louvain_model = Louvain(resolution=resolution, modularity=modularity, random_state=random_state)\n",
    "            labels = louvain_model.fit_predict(similarity_matrix)\n",
    "\n",
    "            # Compute silhouette score\n",
    "            if len(np.unique(labels)) > 1:\n",
    "                silhouette = silhouette_score(data, labels)\n",
    "            else:\n",
    "                silhouette = None  # or a placeholder value, e.g., -1\n",
    "                print(f\"Invalid number of unique labels: {len(np.unique(labels))} for dataset {dataset_name}\")\n",
    "\n",
    "            # Compute V-measure score\n",
    "            v_measure = v_measure_score(true_labels, labels)\n",
    "\n",
    "            # Store results\n",
    "            louvain_results.append({\n",
    "                'dataset': dataset_name,\n",
    "                'resolution': resolution,\n",
    "                'modularity': modularity,\n",
    "                'silhouette_score': silhouette,\n",
    "                'v_measure_score': v_measure,\n",
    "                'cluster_size': len(np.unique(labels))\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(louvain_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution: 1.0, Number of clusters: 3\n",
      "Silhouette Score for resolution 1.0: 0.05235109105706215\n",
      "Resolution: 1.1, Number of clusters: 5000\n",
      "Silhouette score not applicable for resolution 1.1\n",
      "Resolution: 1.2, Number of clusters: 5000\n",
      "Silhouette score not applicable for resolution 1.2\n",
      "Resolution: 1.3, Number of clusters: 5000\n",
      "Silhouette score not applicable for resolution 1.3\n",
      "Resolution: 1.4, Number of clusters: 5000\n",
      "Silhouette score not applicable for resolution 1.4\n"
     ]
    }
   ],
   "source": [
    "# Smaller increments in the range 1 to 1.5\n",
    "resolution_values = [1.0, 1.1, 1.2, 1.3, 1.4]\n",
    "\n",
    "for resolution in resolution_values:\n",
    "    similarity_matrix = create_similarity_matrix(data_to_test)\n",
    "    louvain_model = Louvain(resolution=resolution, random_state=0)\n",
    "    labels = louvain_model.fit_predict(similarity_matrix)\n",
    "\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    print(f\"Resolution: {resolution}, Number of clusters: {num_clusters}\")\n",
    "\n",
    "    # Check if silhouette score is applicable\n",
    "    if 1 < num_clusters < len(data_to_test):\n",
    "        silhouette = silhouette_score(data_to_test, labels)\n",
    "        print(f\"Silhouette Score for resolution {resolution}: {silhouette}\")\n",
    "    else:\n",
    "        print(f\"Silhouette score not applicable for resolution {resolution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters for pge_data: 3\n",
      "Silhouette Score for pge_data: 0.05235109105706215\n"
     ]
    }
   ],
   "source": [
    "# Example for pge_data with resolution = 1.0\n",
    "similarity_matrix = create_similarity_matrix(pge_data)\n",
    "louvain_model = Louvain(resolution=1.0, random_state=0)\n",
    "pge_labels = louvain_model.fit_predict(similarity_matrix)\n",
    "\n",
    "# Check the number of clusters and calculate silhouette score if applicable\n",
    "num_clusters = len(np.unique(pge_labels))\n",
    "print(f\"Number of clusters for pge_data: {num_clusters}\")\n",
    "\n",
    "if 1 < num_clusters < len(pge_data):\n",
    "    silhouette = silhouette_score(pge_data, pge_labels)\n",
    "    print(f\"Silhouette Score for pge_data: {silhouette}\")\n",
    "else:\n",
    "    print(\"Silhouette score not applicable for pge_data\")\n",
    "\n",
    "# Repeat similar steps for other datasets with their optimal resolutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution: 0.1, Number of clusters: 1\n",
      "Resolution: 0.5, Number of clusters: 1\n",
      "Resolution: 1, Number of clusters: 3\n",
      "Silhouette Score for resolution 1: 0.05235109105706215\n",
      "Resolution: 1.5, Number of clusters: 5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 5000. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/code/ML_DS_Case_Studies/Model selection for clustering/Getting_Started/case_study1.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Optional: if you get more than 1 cluster, check the silhouette score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m num_clusters \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     silhouette \u001b[39m=\u001b[39m silhouette_score(data_to_test, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/code/ML_DS_Case_Studies/Model%20selection%20for%20clustering/Getting_Started/case_study1.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSilhouette Score for resolution \u001b[39m\u001b[39m{\u001b[39;00mresolution\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00msilhouette\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:131\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m         X, labels \u001b[39m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:277\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    275\u001b[0m n_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(labels)\n\u001b[1;32m    276\u001b[0m label_freqs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(labels)\n\u001b[0;32m--> 277\u001b[0m check_number_of_labels(\u001b[39mlen\u001b[39m(le\u001b[39m.\u001b[39mclasses_), n_samples)\n\u001b[1;32m    279\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    280\u001b[0m reduce_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    281\u001b[0m     _silhouette_reduce, labels\u001b[39m=\u001b[39mlabels, label_freqs\u001b[39m=\u001b[39mlabel_freqs\n\u001b[1;32m    282\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/cluster/_unsupervised.py:37\u001b[0m, in \u001b[0;36mcheck_number_of_labels\u001b[0;34m(n_labels, n_samples)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m    Number of samples.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m n_labels \u001b[39m<\u001b[39m n_samples:\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNumber of labels is \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. Valid values are 2 to n_samples - 1 (inclusive)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39m%\u001b[39m n_labels\n\u001b[1;32m     40\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels is 5000. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "from sknetwork.clustering import Louvain\n",
    "from scipy import sparse\n",
    "\n",
    "def create_similarity_matrix(data):\n",
    "    distances = pairwise_distances(data)\n",
    "    max_distance = np.max(distances)\n",
    "    similarities = np.exp(-(distances ** 2) / (2. * max_distance ** 2))\n",
    "    return sparse.csr_matrix(similarities)\n",
    "\n",
    "# Select a dataset to experiment with\n",
    "data_to_test = pge_data  # Replace with other datasets as needed\n",
    "\n",
    "# Range of resolution values to test\n",
    "resolution_values = [0.1, 0.5, 1, 1.5, 2]  # Adjust this range based on results\n",
    "\n",
    "for resolution in resolution_values:\n",
    "    similarity_matrix = create_similarity_matrix(data_to_test)\n",
    "    louvain_model = Louvain(resolution=resolution, random_state=0)\n",
    "    labels = louvain_model.fit_predict(similarity_matrix)\n",
    "\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    print(f\"Resolution: {resolution}, Number of clusters: {num_clusters}\")\n",
    "\n",
    "    # Optional: if you get more than 1 cluster, check the silhouette score\n",
    "    if num_clusters > 1:\n",
    "        silhouette = silhouette_score(data_to_test, labels)\n",
    "        print(f\"Silhouette Score for resolution {resolution}: {silhouette}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "selected_index = random.sample(list(np.arange(len(pge_pca_feature))), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pge_pca_feature[selected_index]\n",
    "test_label = labels[selected_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for name in np.unique(labels):\n",
    "    trace = go.Scatter3d(\n",
    "        x=test_data[test_label==name,0],\n",
    "        y=test_data[test_label==name,1],\n",
    "        z=test_data[test_label==name,2],\n",
    "        mode='markers',\n",
    "        name=name,\n",
    "        marker=go.scatter3d.Marker(\n",
    "            size=4,\n",
    "            opacity=0.8\n",
    "        )\n",
    "\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "\n",
    "data = go.Data(traces)\n",
    "layout = go.Layout(\n",
    "            showlegend=True,\n",
    "    scene=go.Scene(\n",
    "                xaxis=go.layout.scene.XAxis(title='PC1'),\n",
    "                yaxis=go.layout.scene.YAxis(title='PC2'),\n",
    "                zaxis=go.layout.scene.ZAxis(title='PC3')\n",
    "                )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.update_layout(\n",
    "    title=\"First 3 pricipal components of PathologyGAN's PCA feature\",\n",
    "    legend_title=\"Legend Title\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sknetwork.clustering import Louvain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create Adjacency matrix for  Louvain clustering\n",
    "from sklearn.metrics import pairwise_distances \n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters = 3, random_state = 0) #GaussianMixture(), AgglomerativeClustering(), Louvain\n",
    "kmeans_assignment = kmeans_model.fit_predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "louvain_model = Louvain(resolution = 0.9, modularity = 'Newman',random_state = 0) \n",
    "adjacency_matrix = sparse.csr_matrix(MinMaxScaler().fit_transform(-pairwise_distances(test_data)))\n",
    "louvain_assignment = louvain_model.fit_transform(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score, v_measure_score\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "# Normalize the PCA features for each dataset\n",
    "datasets = {\n",
    "    'pge': pge_pca_feature,\n",
    "    'resnet50': resnet50_pca_feature,\n",
    "    'inceptionv3': inceptionv3_pca_feature,\n",
    "    'vgg16': vgg16_pca_feature\n",
    "}\n",
    "\n",
    "normalized_datasets = {key: MinMaxScaler().fit_transform(value) for key, value in datasets.items()}\n",
    "\n",
    "# Parameters for Louvain clustering\n",
    "resolutions = [0.9, 1, 0.8]\n",
    "modularity_options = ['Dugue', 'Newman', 'Potts']\n",
    "random_state = 0\n",
    "\n",
    "# Initialize results storage\n",
    "louvain_results = []\n",
    "\n",
    "# Apply Louvain Clustering to each dataset\n",
    "for dataset_name, data in normalized_datasets.items():\n",
    "    for resolution in resolutions:\n",
    "        for modularity in modularity_options:\n",
    "            louvain_model = Louvain(resolution=resolution, modularity=modularity, random_state=random_state)\n",
    "\n",
    "            # Transform distances into similarities\n",
    "            distances = euclidean_distances(data)\n",
    "            max_distance = np.max(distances)\n",
    "            adjacency_matrix = sparse.csr_matrix(np.exp(-(distances ** 2) / (2. * max_distance ** 2)))\n",
    "\n",
    "            # Evaluation Metrics (using Silhouette Score)\n",
    "            silhouette = silhouette_score(data, labels)\n",
    "\n",
    "            # Store results\n",
    "            louvain_results.append({\n",
    "                'dataset': dataset_name,\n",
    "                'resolution': resolution,\n",
    "                'modularity': modularity,\n",
    "                'silhouette_score': silhouette,\n",
    "                'cluster_size': len(np.unique(labels))\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(louvain_results)\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(louvain_results)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=results_df, x='cluster_size', y='silhouette_score', hue='resolution', style='modularity', markers=True)\n",
    "plt.title('Louvain Clustering Performance')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.legend(title='Resolution and Modularity')\n",
    "plt.show()\n",
    "\n",
    "# Summary of Best Configurations\n",
    "best_configurations = results_df.sort_values(by='silhouette_score', ascending=False).groupby(['resolution', 'modularity']).first().reset_index()\n",
    "print(\"Best Configurations by Resolution and Modularity:\")\n",
    "print(best_configurations[['resolution', 'modularity', 'cluster_size', 'silhouette_score']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check out number of clusters/cluster assignment counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of clusters from KMeans: %d and from Louvain: %d'%(np.unique(kmeans_assignment).shape[0],np.unique(louvain_assignment).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_counts = np.unique(kmeans_assignment, return_counts = True)\n",
    "louvain_counts = np.unique(louvain_assignment, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Kmeans assignment counts')\n",
    "pd.DataFrame({'Cluster Index': kmeans_counts[0], 'Number of members':kmeans_counts[1]}).set_index('Cluster Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Louvain assignment counts')\n",
    "pd.DataFrame({'Cluster Index': louvain_counts[0], 'Number of members':louvain_counts[1]}).set_index('Cluster Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assess goodness of fit by silhouette score and cluster homogeneities by V-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_silhouette = silhouette_score(test_data, kmeans_assignment)\n",
    "louvain_silhouette = silhouette_score(test_data, louvain_assignment)\n",
    "kmeans_v_measure = v_measure_score(test_label, kmeans_assignment)\n",
    "louvain_v_measure = v_measure_score(test_label, louvain_assignment)\n",
    "pd.DataFrame({'Metrics': ['silhouette', 'V-measure'], 'Kmeans': [kmeans_silhouette, kmeans_v_measure], 'Louvain':[louvain_silhouette, louvain_v_measure]}).set_index('Metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualise tissue type percentage in two different clustering configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percent(sub_df, attrib):\n",
    "    cnt = sub_df[attrib].count()\n",
    "    output_sub_df = sub_df.groupby(attrib).count()\n",
    "    return (output_sub_df/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulted_cluster_df = pd.DataFrame({'clusterID': kmeans_assignment, 'type': test_label})\n",
    "label_proportion_df = resulted_cluster_df.groupby(['clusterID']).apply(lambda x: calculate_percent(x,'type')).rename(columns={'clusterID':'type_occurrence_percentage'}).reset_index()\n",
    "pivoted_label_proportion_df = pd.pivot_table(label_proportion_df, index = 'clusterID', columns = 'type', values = 'type_occurrence_percentage')\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(20,5))\n",
    "number_of_tile_df = resulted_cluster_df.groupby('clusterID')['type'].count().reset_index().rename(columns={'type':'number_of_tile'})\n",
    "df_idx = pivoted_label_proportion_df.index\n",
    "(pivoted_label_proportion_df*100).loc[df_idx].plot.bar(stacked=True, ax = axes[0] )\n",
    "\n",
    "axes[0].set_ylabel('Percentage of tissue type')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].set_title('Cluster configuration by Kmeans')\n",
    "\n",
    "resulted_cluster_df = pd.DataFrame({'clusterID': louvain_assignment, 'type': test_label})\n",
    "label_proportion_df = resulted_cluster_df.groupby(['clusterID']).apply(lambda x: calculate_percent(x,'type')).rename(columns={'clusterID':'type_occurrence_percentage'}).reset_index()\n",
    "pivoted_label_proportion_df = pd.pivot_table(label_proportion_df, index = 'clusterID', columns = 'type', values = 'type_occurrence_percentage')\n",
    "\n",
    "\n",
    "number_of_tile_df = resulted_cluster_df.groupby('clusterID')['type'].count().reset_index().rename(columns={'type':'number_of_tile'})\n",
    "df_idx = pivoted_label_proportion_df.index\n",
    "(pivoted_label_proportion_df*100).loc[df_idx].plot.bar(stacked=True, ax = axes[1] )\n",
    "\n",
    "axes[1].set_ylabel('Percentage of tissue type')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].set_title('Cluster configuration by Louvain')\n",
    "f.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
